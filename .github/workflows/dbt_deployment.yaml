name: Deploy dbt to GCP

on:
  push:
    branches:
      - main  # Adjust this to your deployment branch

jobs:
  deploy-dbt:
    runs-on: ubuntu-latest
    steps:
    # Step 1: Checkout the code
    - name: Checkout Code
      uses: actions/checkout@v3

    # Step 2: Set up Google Cloud SDK
    - name: Set up Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
        export_default_credentials: true
    - name: Write service account key to JSON file
      run: |
        echo "${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}" > /tmp/service_account_key.json

    # Step 3: Upload dbt project to GCS
    - name: Copy dbt Project to GCS
      run: |
        gsutil -m cp -r ./ gs://${{ secrets.GCP_BUCKET_NAME }}/dbt/
    

    # Step 4: Set up dbt environment (optional, for testing locally)
    - name: Set up dbt environment
      run: |
        python -m venv venv
        source venv/bin/activate
        pip install dbt-bigquery
        dbt --version

    # Step 5: Run dbt commands (optional, for running dbt commands on push)
    - name: Run dbt commands
      run: |
        source venv/bin/activate
        dbt debug --profiles-dir ./profiles
        dbt run --profiles-dir ./profiles
        dbt test --profiles-dir ./profiles
    env:
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/service_account_key.json
    # Step 6: Trigger Airflow DAG (optional, for triggering Airflow directly)
    # - name: Trigger Airflow DAG
    #   run: |
    #     curl -X POST "https://composer.googleapis.com/v1/projects/$GCP_PROJECT_ID/locations/$GCP_REGION/environments/$COMPOSER_ENVIRONMENT_NAME:triggerDag" \
    #     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    #     -H "Content-Type: application/json" \
    #     -d '{
    #           "dag_name": "dbt_pipeline"
    #         }'
